{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e494fff-d6fc-4f24-9a59-cb086f2b92ff",
   "metadata": {},
   "source": [
    "# LLMs from HF\n",
    "\n",
    "This notebooks looks into downloading, loading, and running LLM models from `HuggingFace` using `transformers` 'on-prem'\n",
    "\n",
    "<br>\n",
    "\n",
    "**Consider** that:\n",
    "\n",
    "1. Every time you start your kernel, you need to run the code cells under `env Variables` to set the appropiate environment variables\n",
    "2. When loading the models for the first time, the models will be downloaded first, and this naturally takes more time. If the model has been previously downloaded, it will be directly loaded. For a reference, loading `Llama v3.1 - 8B` in this machine takes 45.1 seconds.\n",
    "3. **These models have a lot of parameters** , and to run inference, the models will be loaded to memory. From HuggingFace (https://huggingface.co/blog/llama31): For inference, the memory requirements depend on the model size and the precision of the weights. Here's a table showing the approximate memory needed for different configurations:\n",
    "\n",
    "| Model Size | FP16     | FP8      | INT4     |\n",
    "|------------|----------|----------|----------|\n",
    "| 8B         | 16 GB    | 8 GB     | 4 GB     |\n",
    "| 70B        | 140 GB   | 70 GB    | 35 GB    |\n",
    "| 405B       | 810 GB   | 405 GB   | 203 GB   |\n",
    "\n",
    "This figures only consider the weights. That is, for `Llama v3.1-70B-FP16`, you neec at least 140 GB! As an example, an H100 node (of 8x H100) has ~640GB of VRAM, so the 405B model would need to be run in a multi-node setup or run at a lower precision (e.g. FP8), which would be the recommended approach.\n",
    "\n",
    "4. The list of \"readily-available\" models can be found [here](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f). In the Llama 3.1 family, the models available are:\n",
    "- meta-llama/Meta-Llama-3.1-8B (& -Instruct)\n",
    "- meta-llama/Meta-Llama-3.1-70B (& -Instruct)\n",
    "- meta-llama/Meta-Llama-3.1-405B (& -Instruct)\n",
    "- meta-llama/Meta-Llama-3.1-405B-FP8 (& -Instruct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd14dc-aeff-4e61-aa3e-bad6bcc23e61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dependencies & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ce609e-8cb6-488f-8a43-f7dcd19ce301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shared-dsrs/LLM\n",
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "aiohttp                   3.9.3\n",
      "aiosignal                 1.3.1\n",
      "alembic                   1.13.1\n",
      "altair                    5.2.0\n",
      "annotated-types           0.6.0\n",
      "anyio                     4.2.0\n",
      "archspec                  0.2.2\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-generator           1.10\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "blinker                   1.7.0\n",
      "bokeh                     3.3.4\n",
      "boltons                   23.1.1\n",
      "Bottleneck                1.3.7\n",
      "Brotli                    1.1.0\n",
      "cached-property           1.5.2\n",
      "certifi                   2024.2.2\n",
      "certipy                   0.1.3\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.1.7\n",
      "cloudpickle               3.0.0\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.1\n",
      "conda                     23.11.0\n",
      "conda-libmamba-solver     24.1.0\n",
      "conda-package-handling    2.2.0\n",
      "conda_package_streaming   0.9.0\n",
      "contourpy                 1.2.0\n",
      "cryptography              42.0.2\n",
      "cycler                    0.12.1\n",
      "Cython                    3.0.8\n",
      "cytoolz                   0.12.3\n",
      "dask                      2024.1.1\n",
      "dask_labextension         7.0.0\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.6\n",
      "distributed               2024.1.1\n",
      "distro                    1.9.0\n",
      "entrypoints               0.4\n",
      "et-xmlfile                1.1.0\n",
      "exceptiongroup            1.2.0\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "filelock                  3.15.4\n",
      "fonttools                 4.48.1\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.2.0\n",
      "gitdb                     4.0.11\n",
      "GitPython                 3.1.41\n",
      "globus-compute-common     0.4.1\n",
      "globus-compute-sdk        2.19.0\n",
      "globus-sdk                3.41.0\n",
      "gmpy2                     2.1.2\n",
      "greenlet                  3.0.3\n",
      "h11                       0.14.0\n",
      "h2                        4.1.0\n",
      "h2o                       3.46.0.1\n",
      "h5py                      3.10.0\n",
      "hpack                     4.0.0\n",
      "httpcore                  1.0.2\n",
      "httpx                     0.26.0\n",
      "huggingface-hub           0.24.5\n",
      "hyperframe                6.0.1\n",
      "idna                      3.6\n",
      "imagecodecs               2024.1.1\n",
      "imageio                   2.34.0\n",
      "importlib-metadata        7.0.1\n",
      "importlib-resources       6.1.1\n",
      "ipykernel                 6.29.2\n",
      "ipympl                    0.9.3\n",
      "ipython                   8.21.0\n",
      "ipython-genutils          0.2.0\n",
      "ipywidgets                8.1.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.14\n",
      "jsonpatch                 1.33\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.0\n",
      "jupyter_core              5.7.1\n",
      "jupyter-events            0.9.0\n",
      "jupyter-lsp               2.2.2\n",
      "jupyter-pluto-proxy       0.1.2\n",
      "jupyter_server            2.12.5\n",
      "jupyter-server-mathjax    0.2.6\n",
      "jupyter_server_proxy      4.1.0\n",
      "jupyter_server_terminals  0.5.2\n",
      "jupyter-telemetry         0.1.0\n",
      "jupyterhub                4.0.2\n",
      "jupyterlab                4.1.1\n",
      "jupyterlab_git            0.50.0\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.2\n",
      "jupyterlab_widgets        3.0.10\n",
      "kiwisolver                1.4.5\n",
      "lazy_loader               0.3\n",
      "libmambapy                1.5.6\n",
      "llvmlite                  0.42.0\n",
      "locket                    1.0.0\n",
      "lz4                       4.3.3\n",
      "Mako                      1.3.2\n",
      "mamba                     1.5.6\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.8.2\n",
      "matplotlib-inline         0.1.6\n",
      "menuinst                  2.0.2\n",
      "mistune                   3.0.2\n",
      "mpmath                    1.3.0\n",
      "msgpack                   1.0.7\n",
      "multidict                 6.0.5\n",
      "munkres                   1.1.4\n",
      "mysql-connector-python    8.4.0\n",
      "nbclassic                 1.0.0\n",
      "nbclient                  0.8.0\n",
      "nbconvert                 7.16.0\n",
      "nbdime                    4.0.1\n",
      "nbformat                  5.9.2\n",
      "nbgitpuller               1.2.1\n",
      "nest_asyncio              1.6.0\n",
      "networkx                  3.2.1\n",
      "notebook                  7.0.7\n",
      "notebook_shim             0.2.3\n",
      "numba                     0.59.0\n",
      "numexpr                   2.9.0\n",
      "numpy                     1.26.4\n",
      "oauthlib                  3.2.2\n",
      "openpyxl                  3.1.2\n",
      "overrides                 7.7.0\n",
      "packaging                 23.2\n",
      "pamela                    1.1.0\n",
      "pandas                    2.2.0\n",
      "pandocfilters             1.5.0\n",
      "parso                     0.8.3\n",
      "partd                     1.4.1\n",
      "patsy                     0.5.6\n",
      "pexpect                   4.9.0\n",
      "pickleshare               0.7.5\n",
      "pika                      1.3.2\n",
      "pillow                    10.2.0\n",
      "pip                       24.0\n",
      "pkgutil_resolve_name      1.3.10\n",
      "platformdirs              4.2.0\n",
      "pluggy                    1.4.0\n",
      "prometheus-client         0.19.0\n",
      "prompt-toolkit            3.0.42\n",
      "protobuf                  4.25.1\n",
      "psutil                    5.9.8\n",
      "psycopg2                  2.9.9\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "py-cpuinfo                9.0.0\n",
      "pyarrow                   15.0.0\n",
      "pyarrow-hotfix            0.6\n",
      "pycosat                   0.6.6\n",
      "pycparser                 2.21\n",
      "pycurl                    7.45.1\n",
      "pydantic                  2.7.1\n",
      "pydantic_core             2.18.2\n",
      "Pygments                  2.17.2\n",
      "PyJWT                     2.8.0\n",
      "pyOpenSSL                 24.0.0\n",
      "pyparsing                 3.1.1\n",
      "PySocks                   1.7.1\n",
      "python-dateutil           2.8.2\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "PyWavelets                1.4.1\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "referencing               0.33.0\n",
      "regex                     2024.7.24\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.17.1\n",
      "rpy2                      3.5.11\n",
      "ruamel.yaml               0.18.6\n",
      "ruamel.yaml.clib          0.2.8\n",
      "safetensors               0.4.3\n",
      "scikit-image              0.22.0\n",
      "scikit-learn              1.4.0\n",
      "scipy                     1.12.0\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.2\n",
      "setuptools                69.0.3\n",
      "simpervisor               1.0.0\n",
      "simplegeneric             0.8.1\n",
      "six                       1.16.0\n",
      "smmap                     5.0.0\n",
      "sniffio                   1.3.0\n",
      "sortedcontainers          2.4.0\n",
      "soupsieve                 2.5\n",
      "SQLAlchemy                2.0.26\n",
      "stack-data                0.6.2\n",
      "statsmodels               0.14.1\n",
      "sympy                     1.12\n",
      "tables                    3.9.2\n",
      "tabulate                  0.9.0\n",
      "tblib                     1.7.0\n",
      "terminado                 0.18.0\n",
      "texttable                 1.7.0\n",
      "threadpoolctl             3.2.0\n",
      "tifffile                  2024.2.12\n",
      "tinycss2                  1.2.1\n",
      "tokenizers                0.19.1\n",
      "tomli                     2.0.1\n",
      "toolz                     0.12.1\n",
      "torch                     2.4.0+cpu\n",
      "torchaudio                2.4.0+cpu\n",
      "torchvision               0.19.0+cpu\n",
      "tornado                   6.3.3\n",
      "tqdm                      4.66.2\n",
      "traitlets                 5.14.1\n",
      "transformers              4.43.3\n",
      "truststore                0.8.0\n",
      "types-python-dateutil     2.8.19.20240106\n",
      "typing_extensions         4.9.0\n",
      "typing-utils              0.1.0\n",
      "tzdata                    2024.1\n",
      "tzlocal                   5.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "websockets                10.3\n",
      "wheel                     0.42.0\n",
      "widgetsnbextension        4.0.10\n",
      "xlrd                      2.0.1\n",
      "xyzservices               2023.10.1\n",
      "yarl                      1.9.4\n",
      "zict                      3.0.0\n",
      "zipp                      3.17.0\n",
      "zstandard                 0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6635015-dfb7-4653-9976-654d3fcab938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.0+cpu)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.19.0+cpu)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.11/site-packages (2.4.0+cpu)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e143-b85e-4539-bcb6-5c79abcc2d1c",
   "metadata": {},
   "source": [
    "## env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9223f79e-f2c1-471e-8367-53ad0cf8da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shared-dsrs/LLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables: ## Needed IF downloading a model ***\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "# Set cache to custom location\n",
    "#### VERY IMPORTANT TO AVOID RUNNING OUT OF MEMMORY IN DSRS JUP HUB ****\n",
    "os.environ['HF_HOME'] = '.'\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0e916-bd44-418e-b940-65c84275aaf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9378f6c1-0036-4cb2-a874-dfd2885f3ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 77.1 ms, total: 473 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", cache_dir=os.environ['HF_HOME'])\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c1b93c-7b23-4544-94aa-8e43adf26743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762ab672241643fc860bbd49bdda254c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed0c8bb781748b6a60b4c06039a8bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9399a7a40c5b4f42a41f1a3396d108dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71275239f1de4eb88170b94acfb073f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69963f06ebb45d48b3aec642997c1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0eb3fe27e44e7f8b1723c6cf33d4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac29035a9bc42bea5a57a6d29ca8013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66f539ea9354e1f8b5204e2c13637eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899ac5b0d9fb48cca47d675a6feae467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load model: 115.47 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", cache_dir=os.environ['HF_HOME'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", cache_dir=os.environ['HF_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3462b693-4b8a-4aa3-ac38-d7d5b6be22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e327b872-6d52-4a69-8d2c-f939164d57d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Tokenization complete.\n",
      "Generating text...\n",
      "Generated text: Once upon a time, a long time ago, there was a girl who loved to write. She loved to write so much that she wrote a story about a girl who loved to write. It was a very special story. It was the story\n",
      "Time taken: 290.09 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "# Set the padding token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64925084-f2cd-4ec7-9f0e-37d4f873e06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  12805,   5304,    264,    892]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dce0564-4244-4795-9a7e-7e2acb8d6c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  12805,   5304,    264,    892,     11,    264,   1317,    892,\n",
       "           4227,     11,   1070,    574,    264,   3828,    889,  10456,    311,\n",
       "           3350,     13,   3005,  10456,    311,   3350,    779,   1790,    430,\n",
       "           1364,   6267,    264,   3446,    922,    264,   3828,    889,  10456,\n",
       "            311,   3350,     13,   1102,    574,    264,   1633,   3361,   3446,\n",
       "             13,   1102,    574,    279,   3446]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6d988d5-1cc7-4555-bdd8-4b2f2a9ddd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(11, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d6e75-c77e-44bb-bc9b-47bf8ad23012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cac3d71-7379-4052-a75f-4409376ff37e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama 3.1 **Instruct** 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6350f03c-c4a1-427b-967f-6874f3dcf0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30658f9d84214136b3ad266d44ebf596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 36.9 s, total: 1min 3s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", cache_dir=os.environ['HF_HOME'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", cache_dir=os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c627e28e-5302-4961-8a4d-f8c3b89113d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Tokenization complete.\n",
      "Generating text...\n",
      "Generated text: Once upon a time, there was a young man named Jack who lived in a small village surrounded by vast fields and dense forests. Jack was a curious and adventurous soul, always eager to explore the unknown and discover new wonders.\n",
      "One day, while\n",
      "CPU times: user 20min 4s, sys: 2.91 s, total: 20min 7s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "# Set the padding token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50, #### Length of generated tokens **\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c5e8d61-abcd-4f33-9443-e6e091dfb7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a787a-a45a-4534-bc18-5cf577aa7683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d40ad-35b4-434a-8c41-68176f2830ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00043054-4327-4e30-95ce-2eab056ffac5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama 3.1 **Instruct** 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9db22f-0526-4b91-930e-a2581512ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/59.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00030.safetensors:   0%|          | 0.00/4.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00030.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B-Instruct\", cache_dir=os.environ['HF_HOME'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B-Instruct\", cache_dir=os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee372b46-cbb3-423e-8694-cc8535ef9c96",
   "metadata": {},
   "source": [
    "![Image](downloading%20llama%203.1%2070B.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c8c5c-b445-4ce4-80a4-65ee16b6b258",
   "metadata": {},
   "source": [
    "# Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f27ea9-dfc0-4f73-a46b-83737cb30816",
   "metadata": {},
   "source": [
    "## Quant\n",
    "This section looks into Quantization using `Quanto` (huggingface.co/docs/transformers/v4.43.3/quantization/quanto), since it **easily integrates with transformers** , and because it is **device agnostic** (e.g CUDA, MPS, CPU). You can also take a look at this [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing).\n",
    "\n",
    "This quantization is **not** serializable with transformers (so, we cannot save with save_pretrained())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b5dc20-d3dc-44b0-97df-c4542c5f1e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting quanto\n",
      "  Using cached quanto-0.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from quanto) (2.4.0+cpu)\n",
      "Collecting ninja (from quanto)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from quanto) (1.26.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from quanto) (0.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.2.0->quanto) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.2.0->quanto) (1.3.0)\n",
      "Using cached quanto-0.2.0-py3-none-any.whl (90 kB)\n",
      "Using cached accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Installing collected packages: ninja, quanto, accelerate\n",
      "Successfully installed accelerate-0.33.0 ninja-1.11.1.1 quanto-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install quanto accelerate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa346772-2391-4bef-b4be-7c92fbad898a",
   "metadata": {},
   "source": [
    "### 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f1ec18-da88-4088-9b07-a97fd50937fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66abb534db4b99a87da9118d5caa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 3min 52s, total: 6min 39s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "hf_home = os.environ['HF_HOME']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=hf_home)\n",
    "\n",
    "# Quant config\n",
    "quantization_config = QuantoConfig(weights=\"float8\") ## ['float8', 'int8', 'int4', 'int2']\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             cache_dir=hf_home, \n",
    "                                             device_map=\"cpu\", \n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6693eb11-dae7-4eef-9a32-27cf1243d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Generating text...\n",
      "How do you compare to GPT4o?\n",
      "I was created by Meta, while GPT-4 was developed by OpenAI. Our training\n",
      "CPU times: user 35min 5s, sys: 20min 57s, total: 56min 2s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"How do you compare to GPT4o\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")#.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Output results\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4088f8-2471-45bd-9992-6552822a4518",
   "metadata": {},
   "source": [
    "### 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d8dd7-ca87-42bf-b7de-06d34ee8cb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c032761ac9984085ace13c57976a6b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "hf_home = os.environ['HF_HOME']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=hf_home)\n",
    "\n",
    "# Quant config\n",
    "quantization_config = QuantoConfig(weights=\"float8\") ## ['float8', 'int8', 'int4', 'int2']\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             cache_dir=hf_home, \n",
    "                                             device_map=\"cpu\", \n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f39136-b5fe-4156-911c-c44482e8d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text = \"How do you compare to GPT4o\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")#.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Output results\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac1f1a-fd6c-4ad6-afd4-9bf6b32a5506",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving the Quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95dc36f-a8a5-48cf-97bd-187a7b388c29",
   "metadata": {},
   "source": [
    "In this case: The model is quantized with QuantizationMethod.QUANTO and is not serializable - check out the warnings from the logger on the traceback to understand the reason why the quantized model is not serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9cb537-ef01-4843-8035-b4d9381fcfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quant_path = \"models--meta-llama--Meta-Llama-3.1-8B-Instruct-FP8\"\n",
    "# model.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fb87b-7eae-49e7-be81-34e1a986be11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AQLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b8dd6f-a0fe-45c1-82e3-9adbc62fc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install aqlm[cpu]#[gpu]\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233db3b7-52b1-4edc-a30e-450b21cd5fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb66385e515049928aee0f2cba6e9d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 42.4 ms, total: 1.23 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "hf_home = os.environ['HF_HOME']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=hf_home)\n",
    "\n",
    "# Quant config\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, cache_dir=hf_home,\n",
    "    torch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad61ea02-cc4c-4af7-9bb0-1dc61628c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Tokenization complete.\n",
      "Generating text...\n",
      "Generated text: Explain the different types of quantization error\n",
      "Quantization error is the difference between the original analog signal and the digital representation of that signal. The types of quantization error are:\n",
      "1. Differential quantization error: This type of error occurs\n",
      "CPU times: user 11min 46s, sys: 2.17 s, total: 11min 49s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Explain the different types of quantization\"\n",
    "\n",
    "# Set the padding token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = quantized_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50, #### Length of generated tokens **\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce406caf-b1e6-462d-97e4-e00fc353a5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6829fc94-6691-4eb1-9993-22a1dd0fddff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hqq\n",
      "  Downloading hqq-0.1.8.tar.gz (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.11/site-packages (from hqq) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.11/site-packages (from hqq) (4.66.2)\n",
      "Collecting einops (from hqq)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (from hqq) (0.33.0)\n",
      "Requirement already satisfied: transformers>=4.36.1 in /opt/conda/lib/python3.11/site-packages (from hqq) (4.43.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (from hqq) (0.24.5)\n",
      "Collecting termcolor (from hqq)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting bitblas (from hqq)\n",
      "  Downloading bitblas-0.0.1.dev13-py3-none-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (0.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->hqq) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->hqq) (4.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->hqq) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate->hqq) (2.4.0+cpu)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (1.16.0)\n",
      "Collecting cpplint (from bitblas->hqq)\n",
      "  Downloading cpplint-1.6.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (3.0.8)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (5.1.1)\n",
      "Collecting docutils (from bitblas->hqq)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting dtlib (from bitblas->hqq)\n",
      "  Downloading dtlib-0.0.0.dev2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pytest>=6.2.4 (from bitblas->hqq)\n",
      "  Downloading pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pytest-xdist>=2.2.1 (from bitblas->hqq)\n",
      "  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->hqq)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (3.0.0)\n",
      "Collecting ml-dtypes (from bitblas->hqq)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (1.12.0)\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (6.3.3)\n",
      "Collecting thefuzz (from bitblas->hqq)\n",
      "  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (0.9.0)\n",
      "Collecting iniconfig (from pytest>=6.2.4->bitblas->hqq)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=6.2.4->bitblas->hqq)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting execnet>=2.1 (from pytest-xdist>=2.2.1->bitblas->hqq)\n",
      "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate->hqq) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate->hqq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate->hqq) (3.1.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi->bitblas->hqq) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (2024.2.2)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz->bitblas->hqq)\n",
      "  Downloading rapidfuzz-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate->hqq) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate->hqq) (1.3.0)\n",
      "Downloading bitblas-0.0.1.dev13-py3-none-manylinux1_x86_64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading pytest-8.3.2-py3-none-any.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading cpplint-1.6.1-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dtlib-0.0.0.dev2-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
      "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading rapidfuzz-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Building wheels for collected packages: hqq\n",
      "  Building wheel for hqq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hqq: filename=hqq-0.1.8-py3-none-any.whl size=61842 sha256=d33e640649702bf0004823b37b9c6df778611338a4c168c59be8515f72a9c3bb\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/48/c9/d7/e9a4d0e9ff9ec96c2b629713b55e0221fee3f1ca41071b0a5e\n",
      "Successfully built hqq\n",
      "Installing collected packages: cpplint, typing-extensions, termcolor, rapidfuzz, pluggy, ml-dtypes, iniconfig, execnet, einops, dtlib, docutils, thefuzz, pytest, pytest-xdist, bitblas, hqq\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.4.0\n",
      "    Uninstalling pluggy-1.4.0:\n",
      "      Successfully uninstalled pluggy-1.4.0\n",
      "Successfully installed bitblas-0.0.1.dev13 cpplint-1.6.1 docutils-0.21.2 dtlib-0.0.0.dev2 einops-0.8.0 execnet-2.1.1 hqq-0.1.8 iniconfig-2.0.0 ml-dtypes-0.4.0 pluggy-1.5.0 pytest-8.3.2 pytest-xdist-3.6.1 rapidfuzz-3.9.5 termcolor-2.4.0 thefuzz-0.22.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284d7dc2-3463-4e0b-978a-3b59616acbe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'HQQBaseQuantizeConfig' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, HqqConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Method 1: all linear layers will use the same quantization config\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m quant_config  \u001b[38;5;241m=\u001b[39m \u001b[43mHqqConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#axis=0 is used by default\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/quantization_config.py:238\u001b[0m, in \u001b[0;36mHqqConfig.__init__\u001b[0;34m(self, nbits, group_size, quant_zero, quant_scale, offload_meta, view_as_float, axis, dynamic_config, skip_modules, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config[key] \u001b[38;5;241m=\u001b[39m HQQBaseQuantizeConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdynamic_config[key])\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m \u001b[43mHQQBaseQuantizeConfig\u001b[49m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnbits\u001b[39m\u001b[38;5;124m\"\u001b[39m: nbits,\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: group_size,\n\u001b[1;32m    242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m: quant_zero,\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m: quant_scale,\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m: offload_meta,\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mview_as_float\u001b[39m\u001b[38;5;124m\"\u001b[39m: view_as_float,\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m: axis,\n\u001b[1;32m    247\u001b[0m         }\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;241m=\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mHQQ\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_modules \u001b[38;5;241m=\u001b[39m skip_modules\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'HQQBaseQuantizeConfig' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n",
    "\n",
    "# Method 1: all linear layers will use the same quantization config\n",
    "quant_config  = HqqConfig(nbits=8, quant_zero=False, quant_scale=False) #axis=0 is used by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a6f24-60e4-46f2-8473-c4731cdcca05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
