{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e494fff-d6fc-4f24-9a59-cb086f2b92ff",
   "metadata": {},
   "source": [
    "# LLMs from HF\n",
    "\n",
    "This notebooks looks into downloading, loading, and running pre-trained LLM models from `HuggingFace` using `transformers` 'on-premises'.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Consider** that:\n",
    "\n",
    "1. Every time you start your kernel, you need to run the code cells under `env Variables` to set the appropiate environment variables\n",
    "2. When loading the models for the first time, the models will be downloaded first, and this naturally takes more time. If the model has been previously downloaded, it will be directly loaded. For a reference, loading `Llama v3.1 - 8B` in this machine takes 45.1 seconds.\n",
    "3. **These models have a lot of parameters** , and to run inference, the models will be loaded to memory. From HuggingFace (https://huggingface.co/blog/llama31): For inference, the memory requirements depend on the model size and the precision of the weights. Here's a table showing the approximate memory needed for different configurations:\n",
    "\n",
    "| Model Size | FP16     | FP8      | INT4     |\n",
    "|------------|----------|----------|----------|\n",
    "| 8B         | 16 GB    | 8 GB     | 4 GB     |\n",
    "| 70B        | 140 GB   | 70 GB    | 35 GB    |\n",
    "| 405B       | 810 GB   | 405 GB   | 203 GB   |\n",
    "\n",
    "This figures only consider the weights. That is, for `Llama v3.1-70B-FP16`, you neec at least 140 GB! As an example, an H100 node (of 8x H100) has ~640GB of VRAM, so the 405B model would need to be run in a multi-node setup or run at a lower precision (e.g. FP8), which would be the recommended approach.\n",
    "\n",
    "4. The list of \"readily-available\" models can be found [here](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f). In the Llama 3.1 family, the models available are:\n",
    "- meta-llama/Meta-Llama-3.1-8B (& -Instruct)\n",
    "- meta-llama/Meta-Llama-3.1-70B (& -Instruct)\n",
    "- meta-llama/Meta-Llama-3.1-405B (& -Instruct)\n",
    "- meta-llama/Meta-Llama-3.1-405B-FP8 (& -Instruct)\n",
    "\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd14dc-aeff-4e61-aa3e-bad6bcc23e61",
   "metadata": {},
   "source": [
    "## Dependencies & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce609e-8cb6-488f-8a43-f7dcd19ce301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6635015-dfb7-4653-9976-654d3fcab938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e143-b85e-4539-bcb6-5c79abcc2d1c",
   "metadata": {},
   "source": [
    "## env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9223f79e-f2c1-471e-8367-53ad0cf8da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shared-dsrs/LLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Print the HF_TOKEN\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Set cache to custom location\n",
    "#### VERY IMPORTANT TO AVOID RUNNING OUT OF MEMMORY IN DSRS JUP HUB ****\n",
    "os.environ['HF_HOME'] = '.'\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0e916-bd44-418e-b940-65c84275aaf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9378f6c1-0036-4cb2-a874-dfd2885f3ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 77.1 ms, total: 473 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", cache_dir=os.environ['HF_HOME'])\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c1b93c-7b23-4544-94aa-8e43adf26743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762ab672241643fc860bbd49bdda254c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed0c8bb781748b6a60b4c06039a8bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9399a7a40c5b4f42a41f1a3396d108dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71275239f1de4eb88170b94acfb073f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69963f06ebb45d48b3aec642997c1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0eb3fe27e44e7f8b1723c6cf33d4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac29035a9bc42bea5a57a6d29ca8013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66f539ea9354e1f8b5204e2c13637eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899ac5b0d9fb48cca47d675a6feae467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load model: 115.47 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", cache_dir=os.environ['HF_HOME'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", cache_dir=os.environ['HF_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3462b693-4b8a-4aa3-ac38-d7d5b6be22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e327b872-6d52-4a69-8d2c-f939164d57d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Tokenization complete.\n",
      "Generating text...\n",
      "Generated text: Once upon a time, a long time ago, there was a girl who loved to write. She loved to write so much that she wrote a story about a girl who loved to write. It was a very special story. It was the story\n",
      "Time taken: 290.09 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "# Set the padding token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64925084-f2cd-4ec7-9f0e-37d4f873e06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  12805,   5304,    264,    892]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dce0564-4244-4795-9a7e-7e2acb8d6c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  12805,   5304,    264,    892,     11,    264,   1317,    892,\n",
       "           4227,     11,   1070,    574,    264,   3828,    889,  10456,    311,\n",
       "           3350,     13,   3005,  10456,    311,   3350,    779,   1790,    430,\n",
       "           1364,   6267,    264,   3446,    922,    264,   3828,    889,  10456,\n",
       "            311,   3350,     13,   1102,    574,    264,   1633,   3361,   3446,\n",
       "             13,   1102,    574,    279,   3446]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6d988d5-1cc7-4555-bdd8-4b2f2a9ddd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(11, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d6e75-c77e-44bb-bc9b-47bf8ad23012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cac3d71-7379-4052-a75f-4409376ff37e",
   "metadata": {},
   "source": [
    "## Llama 3.1 **Instruct** 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6350f03c-c4a1-427b-967f-6874f3dcf0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30658f9d84214136b3ad266d44ebf596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 36.9 s, total: 1min 3s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", cache_dir=os.environ['HF_HOME'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", cache_dir=os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c627e28e-5302-4961-8a4d-f8c3b89113d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Tokenization complete.\n",
      "Generating text...\n",
      "Generated text: Once upon a time, there was a young man named Jack who lived in a small village surrounded by vast fields and dense forests. Jack was a curious and adventurous soul, always eager to explore the unknown and discover new wonders.\n",
      "One day, while\n",
      "CPU times: user 20min 4s, sys: 2.91 s, total: 20min 7s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "# Set the padding token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50, #### Length of generated tokens **\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c5e8d61-abcd-4f33-9443-e6e091dfb7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a787a-a45a-4534-bc18-5cf577aa7683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d40ad-35b4-434a-8c41-68176f2830ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00043054-4327-4e30-95ce-2eab056ffac5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama 3.1 **Instruct** 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9db22f-0526-4b91-930e-a2581512ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/59.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00030.safetensors:   0%|          | 0.00/4.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00030.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B-Instruct\", cache_dir=os.environ['HF_HOME'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B-Instruct\", cache_dir=os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee372b46-cbb3-423e-8694-cc8535ef9c96",
   "metadata": {},
   "source": [
    "![Image](downloading%20llama%203.1%2070B.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c8c5c-b445-4ce4-80a4-65ee16b6b258",
   "metadata": {},
   "source": [
    "# Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f27ea9-dfc0-4f73-a46b-83737cb30816",
   "metadata": {},
   "source": [
    "## Quant\n",
    "This section looks into Quantization using `Quanto` (huggingface.co/docs/transformers/v4.43.3/quantization/quanto), since it **easily integrates with transformers** , and because it is **device agnostic** (e.g CUDA, MPS, CPU). You can also take a look at this [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing).\n",
    "\n",
    "This quantization is **not** serializable with transformers (so, we cannot save with save_pretrained())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5dc20-d3dc-44b0-97df-c4542c5f1e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install quanto accelerate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa346772-2391-4bef-b4be-7c92fbad898a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f1ec18-da88-4088-9b07-a97fd50937fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66abb534db4b99a87da9118d5caa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 3min 52s, total: 6min 39s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "hf_home = os.environ['HF_HOME']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=hf_home)\n",
    "\n",
    "# Quant config\n",
    "quantization_config = QuantoConfig(weights=\"float8\") ## ['float8', 'int8', 'int4', 'int2']\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             cache_dir=hf_home, \n",
    "                                             device_map=\"cpu\", \n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6693eb11-dae7-4eef-9a32-27cf1243d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Generating text...\n",
      "How do you compare to GPT4o?\n",
      "I was created by Meta, while GPT-4 was developed by OpenAI. Our training\n",
      "CPU times: user 35min 5s, sys: 20min 57s, total: 56min 2s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"How do you compare to GPT4o\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")#.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Output results\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4088f8-2471-45bd-9992-6552822a4518",
   "metadata": {},
   "source": [
    "### 70B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4d8dd7-ca87-42bf-b7de-06d34ee8cb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608337703b5947c4a1c80e33d28c0030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8b0c8388ca4b71a3befe1d74d38911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 53s, sys: 46min 1s, total: 1h 13min 54s\n",
      "Wall time: 21min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "hf_home = os.environ['HF_HOME']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=hf_home)\n",
    "\n",
    "# Quant config\n",
    "quantization_config = QuantoConfig(weights=\"int4\") ## ['float8', 'int8', 'int4', 'int2']\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             cache_dir=hf_home, \n",
    "                                             device_map=\"cpu\", \n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceff357-06b8-4ab5-bc1e-8c3a32d301b4",
   "metadata": {},
   "source": [
    "![Image](70B_quantized_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f39136-b5fe-4156-911c-c44482e8d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Generating text...\n",
      "How do you compare to GPT4oL?\n",
      "CPU times: user 38min 25s, sys: 1h 13min 8s, total: 1h 51min 33s\n",
      "Wall time: 28min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"How do you compare to GPT4o\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")#.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Output results\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7778eef-eb5e-46b7-9d42-b856b610f90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "How\n",
      " do\n",
      " you\n",
      " compare\n",
      " to\n",
      " G\n",
      "PT\n",
      "4\n",
      "o\n",
      "L\n",
      "?\n",
      "<|eot_id|>\n",
      "\n",
      "Total # of tokens generated: 13 (including special tokens..)\n"
     ]
    }
   ],
   "source": [
    "for token in range(outputs[0].shape[0]):\n",
    "    print(tokenizer.decode(outputs[0][token]))\n",
    "    \n",
    "print(f'\\nTotal # of tokens generated: {outputs[0].shape[0]} (including special tokens..)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8c7ee73-703d-4061-9729-8856362cf460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Generating text...\n",
      "Teach me Quantization for DL in 50 words.\n",
      "\n",
      "Quantization in Deep Learning (DL) is a technique to reduce the precision of model weights and activations from floating-point numbers (e.g., float32) to integers (e.g.,\n",
      "CPU times: user 7h 44min 26s, sys: 15h 12min 22s, total: 22h 56min 49s\n",
      "Wall time: 5h 49min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Teach me Quantization for DL in 50 words\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")#.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = model.generate(**inputs, max_length=50) #### Length of generated tokens **\n",
    "\n",
    "# Output results\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac1f1a-fd6c-4ad6-afd4-9bf6b32a5506",
   "metadata": {},
   "source": [
    "### Saving the Quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95dc36f-a8a5-48cf-97bd-187a7b388c29",
   "metadata": {},
   "source": [
    "In this case: The model is quantized with QuantizationMethod.QUANTO and is not serializable - check out the warnings from the logger on the traceback to understand the reason why the quantized model is not serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9cb537-ef01-4843-8035-b4d9381fcfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quant_path = \"models--meta-llama--Meta-Llama-3.1-8B-Instruct-FP8\"\n",
    "# model.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fb87b-7eae-49e7-be81-34e1a986be11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AQLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b8dd6f-a0fe-45c1-82e3-9adbc62fc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install aqlm[cpu]#[gpu]\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233db3b7-52b1-4edc-a30e-450b21cd5fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb66385e515049928aee0f2cba6e9d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 42.4 ms, total: 1.23 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "hf_home = os.environ['HF_HOME']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=hf_home)\n",
    "\n",
    "# Quant config\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, cache_dir=hf_home,\n",
    "    torch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad61ea02-cc4c-4af7-9bb0-1dc61628c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input...\n",
      "Tokenization complete.\n",
      "Generating text...\n",
      "Generated text: Explain the different types of quantization error\n",
      "Quantization error is the difference between the original analog signal and the digital representation of that signal. The types of quantization error are:\n",
      "1. Differential quantization error: This type of error occurs\n",
      "CPU times: user 11min 46s, sys: 2.17 s, total: 11min 49s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Explain the different types of quantization\"\n",
    "\n",
    "# Set the padding token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input text with attention mask\n",
    "print(\"Tokenizing input...\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating text...\")\n",
    "outputs = quantized_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50, #### Length of generated tokens **\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce406caf-b1e6-462d-97e4-e00fc353a5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6829fc94-6691-4eb1-9993-22a1dd0fddff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hqq\n",
      "  Downloading hqq-0.1.8.tar.gz (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.11/site-packages (from hqq) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.11/site-packages (from hqq) (4.66.2)\n",
      "Collecting einops (from hqq)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (from hqq) (0.33.0)\n",
      "Requirement already satisfied: transformers>=4.36.1 in /opt/conda/lib/python3.11/site-packages (from hqq) (4.43.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (from hqq) (0.24.5)\n",
      "Collecting termcolor (from hqq)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting bitblas (from hqq)\n",
      "  Downloading bitblas-0.0.1.dev13-py3-none-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.36.1->hqq) (0.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->hqq) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->hqq) (4.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->hqq) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate->hqq) (2.4.0+cpu)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (1.16.0)\n",
      "Collecting cpplint (from bitblas->hqq)\n",
      "  Downloading cpplint-1.6.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (3.0.8)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (5.1.1)\n",
      "Collecting docutils (from bitblas->hqq)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting dtlib (from bitblas->hqq)\n",
      "  Downloading dtlib-0.0.0.dev2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pytest>=6.2.4 (from bitblas->hqq)\n",
      "  Downloading pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pytest-xdist>=2.2.1 (from bitblas->hqq)\n",
      "  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->hqq)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (3.0.0)\n",
      "Collecting ml-dtypes (from bitblas->hqq)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (1.12.0)\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (6.3.3)\n",
      "Collecting thefuzz (from bitblas->hqq)\n",
      "  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.11/site-packages (from bitblas->hqq) (0.9.0)\n",
      "Collecting iniconfig (from pytest>=6.2.4->bitblas->hqq)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=6.2.4->bitblas->hqq)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting execnet>=2.1 (from pytest-xdist>=2.2.1->bitblas->hqq)\n",
      "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate->hqq) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate->hqq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate->hqq) (3.1.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi->bitblas->hqq) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.36.1->hqq) (2024.2.2)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz->bitblas->hqq)\n",
      "  Downloading rapidfuzz-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate->hqq) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate->hqq) (1.3.0)\n",
      "Downloading bitblas-0.0.1.dev13-py3-none-manylinux1_x86_64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading pytest-8.3.2-py3-none-any.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading cpplint-1.6.1-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dtlib-0.0.0.dev2-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
      "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading rapidfuzz-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Building wheels for collected packages: hqq\n",
      "  Building wheel for hqq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hqq: filename=hqq-0.1.8-py3-none-any.whl size=61842 sha256=d33e640649702bf0004823b37b9c6df778611338a4c168c59be8515f72a9c3bb\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/48/c9/d7/e9a4d0e9ff9ec96c2b629713b55e0221fee3f1ca41071b0a5e\n",
      "Successfully built hqq\n",
      "Installing collected packages: cpplint, typing-extensions, termcolor, rapidfuzz, pluggy, ml-dtypes, iniconfig, execnet, einops, dtlib, docutils, thefuzz, pytest, pytest-xdist, bitblas, hqq\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.4.0\n",
      "    Uninstalling pluggy-1.4.0:\n",
      "      Successfully uninstalled pluggy-1.4.0\n",
      "Successfully installed bitblas-0.0.1.dev13 cpplint-1.6.1 docutils-0.21.2 dtlib-0.0.0.dev2 einops-0.8.0 execnet-2.1.1 hqq-0.1.8 iniconfig-2.0.0 ml-dtypes-0.4.0 pluggy-1.5.0 pytest-8.3.2 pytest-xdist-3.6.1 rapidfuzz-3.9.5 termcolor-2.4.0 thefuzz-0.22.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284d7dc2-3463-4e0b-978a-3b59616acbe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'HQQBaseQuantizeConfig' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, HqqConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Method 1: all linear layers will use the same quantization config\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m quant_config  \u001b[38;5;241m=\u001b[39m \u001b[43mHqqConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#axis=0 is used by default\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/quantization_config.py:238\u001b[0m, in \u001b[0;36mHqqConfig.__init__\u001b[0;34m(self, nbits, group_size, quant_zero, quant_scale, offload_meta, view_as_float, axis, dynamic_config, skip_modules, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config[key] \u001b[38;5;241m=\u001b[39m HQQBaseQuantizeConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdynamic_config[key])\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m \u001b[43mHQQBaseQuantizeConfig\u001b[49m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnbits\u001b[39m\u001b[38;5;124m\"\u001b[39m: nbits,\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: group_size,\n\u001b[1;32m    242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m: quant_zero,\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m: quant_scale,\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m: offload_meta,\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mview_as_float\u001b[39m\u001b[38;5;124m\"\u001b[39m: view_as_float,\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m: axis,\n\u001b[1;32m    247\u001b[0m         }\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;241m=\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mHQQ\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_modules \u001b[38;5;241m=\u001b[39m skip_modules\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'HQQBaseQuantizeConfig' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n",
    "\n",
    "# Method 1: all linear layers will use the same quantization config\n",
    "quant_config  = HqqConfig(nbits=8, quant_zero=False, quant_scale=False) #axis=0 is used by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a6f24-60e4-46f2-8473-c4731cdcca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a627c3-850a-4d48-a4c1-4d544003697b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
